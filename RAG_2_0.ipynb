{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-community\n",
        "! pip install chromadb\n",
        "! pip install pypdf\n",
        "! pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uTzYRP5hK8Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedder"
      ],
      "metadata": {
        "id": "hXYp0D4FR18l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GSFcr9CgHKj_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_repo(repo_url, destination=None):\n",
        "  command = [\"git\", \"clone\", repo_url]\n",
        "  if destination: command.append(destination)\n",
        "  subprocess.run(command, check=True)\n",
        "def get_all_file_paths(directory):\n",
        "  file_paths = []\n",
        "  for root, _, files in os.walk(directory):\n",
        "    for file in files: file_paths.append(os.path.join(root, file))\n",
        "  return file_paths"
      ],
      "metadata": {
        "id": "GbeyZ3ttHpvp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_files(directories, files):\n",
        "  def should_keep(file_path):\n",
        "    return not any(f\"/{dir}/\" in file_path for dir in directories)\n",
        "  return list(filter(should_keep, files))"
      ],
      "metadata": {
        "id": "1kHvXgNAU4vE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clone_repo(\"https://github.com/cokelaer/fitter\")"
      ],
      "metadata": {
        "id": "3Y5yDEKMNAzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder:\n",
        "  def __init__(self, directories, directories_filter, db_name, refresh_db=False, metadata={}):\n",
        "    self.directories = directories\n",
        "    self.directories_filter = directories_filter\n",
        "    self.db_name    = db_name\n",
        "    self.docs       = []\n",
        "    self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    self.embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device' : 'cpu'})\n",
        "    self.metadata = metadata\n",
        "    if refresh_db: self.load_files()\n",
        "    print(\"Done Setup\")\n",
        "  def load_files(self):\n",
        "    for dir in self.directories: self.load_directory(dir)\n",
        "    docs = self.text_splitter.split_documents(self.docs)\n",
        "    vectorstore = Chroma.from_documents(docs, self.embedding_function, persist_directory=self.db_name)\n",
        "    print(\"Done file load\")\n",
        "  def load_directory(self, directory_path):\n",
        "    file_list = get_all_file_paths(directory_path)\n",
        "    file_list = filter_files(self.directories_filter, file_list)\n",
        "    for f in file_list:\n",
        "      extension = f.split(\"/\")[-1].split(\".\")[-1]\n",
        "      document = None\n",
        "      if extension == \"pdf\": document = PyPDFLoader(f).load()\n",
        "      elif extension in [\"png\", \"jpg\", \"jpeg\", \"exe\", \"bat\"]: continue\n",
        "      else: document = TextLoader(f).load()\n",
        "      document[0].metadata = document[0].metadata | self.metadata\n",
        "      print(document[0].metadata)\n",
        "      self.docs.extend(document)"
      ],
      "metadata": {
        "id": "t1jS4dnyLymk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directories = [\"./fitter\", \"./input_files\"]\n",
        "directories_filter = [\".git\", \".github\"]\n",
        "metadata = {\"user_id\" : \"asdgw2dsag\"}\n",
        "emb = Embedder(directories, directories_filter, \"./new_db\", True, metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE-TocAsJZQ_",
        "outputId": "2cc168ee-cde6-4d35-97c0-3030b208eb7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d8ed4533a5b5>:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  self.embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device' : 'cpu'})\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': './fitter/poetry.lock', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/.pre-commit-config.yaml', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/.readthedocs.yml', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/pyproject.toml', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/README.rst', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/LICENSE', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/faqs.rst', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/tuto.rst', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/Makefile', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/contrib.rst', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/index.rst', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/conf.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/requirements.txt', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/data.csv', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/references.rst', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/source/conf.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/doc/source/_static/copybutton.js', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/test/__init__.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/test/test_histfit.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/test/test_main.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/test/test_fitter.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/src/fitter/main.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/src/fitter/__init__.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/src/fitter/fitter.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './fitter/src/fitter/histfit.py', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './input_files/Calypso_102.md', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './input_files/Calypso_101.md', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './input_files/Calypso_104.md', 'user_id': 'asdgw2dsag'}\n",
            "{'source': './input_files/Calypso_103.md', 'user_id': 'asdgw2dsag'}\n",
            "Done file load\n",
            "Done Setup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "a3gJrp-EZqQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import google.generativeai as genai\n",
        "import json"
      ],
      "metadata": {
        "id": "wpPNq1byQPee"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = \"AIzaSyBaBCyUV9Rj6KgeAYoXdl09I3kV7JOJbfg\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "GDcNQLjwZteX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGHandler:\n",
        "  def __init__(self, model, db_name):\n",
        "    self.model = model\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device' : 'cpu'})\n",
        "    self.vector_db = Chroma(persist_directory=db_name, embedding_function=embedding_function)\n",
        "  def _generate_rag_prompt(self, query, context):\n",
        "    escaped = context.replace(\"'\", \"\").replace('\"', '').replace(\"\\n\", \" \")\n",
        "    open_br = \"{\"\n",
        "    close_br = \"}\"\n",
        "    prompt = (f\"\"\"\n",
        "    You are a helpful and informative bot that answers questions using text from reference context included below. \\\n",
        "    Be sure to respond in a complete sentence, being comprenhensive, including all relevant background information. \\\n",
        "    However, you are talinkg to a non-technical audience, so be sure to break down complicated concepts and \\\n",
        "    strike friendly and conversational tone. \\\n",
        "    If the context is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "    USER QUESTION: '{query}'\n",
        "    CONTEXT: '{context}'\n",
        "\n",
        "    ANSWER:\n",
        "    Each context information will have some metadata at the end of the object. \\\n",
        "    If the information from given file is relevant, use the metadata to refence \\\n",
        "    by adding the source to a list in the sources list. Use the following format, \\\n",
        "    as a json object:\n",
        "\n",
        "    {open_br}\n",
        "      answer : <your response> // string\n",
        "      sources : [\n",
        "        {open_br}\n",
        "          file_name: <Name of file>, // string\n",
        "          path: <path to source> // string\n",
        "        {close_br}\n",
        "      ] // list of path sources\n",
        "    {close_br}\n",
        "    \"\"\")\n",
        "    return prompt\n",
        "  def _get_relevant_context_from_db(self, query, metadata):\n",
        "    context = \"\"\n",
        "    search_results = self.vector_db.similarity_search(query, k=6, filter=metadata)\n",
        "    for result in search_results:\n",
        "      context += result.page_content + \"\\n\"\n",
        "      context += f\"{result.metadata}\" + \"\\n\"\n",
        "    return context\n",
        "  def _generate_answer(self, prompt):\n",
        "    answer = self.model.generate_content(prompt)\n",
        "    return answer.text\n",
        "  def query(self, query, metadata={}):\n",
        "    context = self._get_relevant_context_from_db(query, metadata)\n",
        "    prompt  = self._generate_rag_prompt(query, context)\n",
        "    answer  = self._generate_answer(prompt)\n",
        "    answer  = json.loads(answer)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "kJv5SWlZZu1_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_handler = RAGHandler(model, \"./new_db\")"
      ],
      "metadata": {
        "id": "LrSfQJ89Zw_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c4523d-0b79-41e8-e914-f96d61839a1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5f1025d5da97>:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
            "  self.vector_db = Chroma(persist_directory=db_name, embedding_function=embedding_function)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = {\"user_id\" : \"asdgw2dsag\"}\n",
        "query = rag_handler.query(\"According to the documentation, what's the main purpose of the standard model?\", metadata)"
      ],
      "metadata": {
        "id": "OxS4JKM0Zzcf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPe9-rsiZ0X4",
        "outputId": "577f7567-2bf8-44a5-b4a5-f6c955e89fe8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'The Calypso Standard Reference Model (CSRM) is a pre-configured and extensible data model that helps standardize how financial instruments are represented. This makes it easier to integrate with other systems and reduces the need for custom development. It also includes tools and frameworks for managing trades, valuations, and risks across various asset classes.', 'sources': [{'file_name': 'Calypso_104.md', 'path': './input_files/Calypso_104.md'}]}\n"
          ]
        }
      ]
    }
  ]
}