{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-community\n",
        "! pip install chromadb\n",
        "! pip install pypdf\n",
        "! pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uTzYRP5hK8Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedder"
      ],
      "metadata": {
        "id": "hXYp0D4FR18l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GSFcr9CgHKj_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_repo(repo_url, destination=None):\n",
        "  command = [\"git\", \"clone\", repo_url]\n",
        "  if destination: command.append(destination)\n",
        "  subprocess.run(command, check=True)\n",
        "def get_all_file_paths(directory):\n",
        "  file_paths = []\n",
        "  for root, _, files in os.walk(directory):\n",
        "    for file in files: file_paths.append(os.path.join(root, file))\n",
        "  return file_paths"
      ],
      "metadata": {
        "id": "GbeyZ3ttHpvp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_files(directories, files):\n",
        "  def should_keep(file_path):\n",
        "    return not any(f\"/{dir}/\" in file_path for dir in directories)\n",
        "  return list(filter(should_keep, files))"
      ],
      "metadata": {
        "id": "1kHvXgNAU4vE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clone_repo(\"https://github.com/cokelaer/fitter\")"
      ],
      "metadata": {
        "id": "3Y5yDEKMNAzv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder:\n",
        "  def __init__(self, directories, directories_filter, db_name, refresh_db=False):\n",
        "    self.directories = directories\n",
        "    self.directories_filter = directories_filter\n",
        "    self.db_name    = db_name\n",
        "    self.docs       = []\n",
        "    self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    self.embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device' : 'cpu'})\n",
        "    if refresh_db: self.load_files()\n",
        "    print(\"Done Setup\")\n",
        "  def load_files(self):\n",
        "    for dir in self.directories: self.load_directory(dir)\n",
        "    docs = self.text_splitter.split_documents(self.docs)\n",
        "    vectorstore = Chroma.from_documents(docs, self.embedding_function, persist_directory=self.db_name)\n",
        "    print(\"Done file load\")\n",
        "  def load_directory(self, directory_path):\n",
        "    file_list = get_all_file_paths(directory_path)\n",
        "    file_list = filter_files(self.directories_filter, file_list)\n",
        "    for f in file_list:\n",
        "      extension = f.split(\"/\")[-1].split(\".\")[-1]\n",
        "      print(f)\n",
        "      if extension == \"pdf\": self.docs.extend(PyPDFLoader(f).load())\n",
        "      elif extension in [\"png\", \"jpg\", \"jpeg\", \"exe\", \"bat\"]: continue\n",
        "      else:self.docs.extend(TextLoader(f).load())"
      ],
      "metadata": {
        "id": "t1jS4dnyLymk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directories = [\"./fitter\", \"./input_files\"]\n",
        "directories_filter = [\".git\", \".github\"]\n",
        "emb = Embedder(directories, directories_filter, \"./new_db\", True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE-TocAsJZQ_",
        "outputId": "82da22e9-7111-4f9b-960a-a916ca5b87fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./fitter/.readthedocs.yml\n",
            "./fitter/.pre-commit-config.yaml\n",
            "./fitter/pyproject.toml\n",
            "./fitter/LICENSE\n",
            "./fitter/README.rst\n",
            "./fitter/poetry.lock\n",
            "./fitter/test/__init__.py\n",
            "./fitter/test/test_histfit.py\n",
            "./fitter/test/test_fitter.py\n",
            "./fitter/test/test_main.py\n",
            "./fitter/doc/Makefile\n",
            "./fitter/doc/tuto.rst\n",
            "./fitter/doc/references.rst\n",
            "./fitter/doc/index.rst\n",
            "./fitter/doc/contrib.rst\n",
            "./fitter/doc/data.csv\n",
            "./fitter/doc/faqs.rst\n",
            "./fitter/doc/conf.py\n",
            "./fitter/doc/requirements.txt\n",
            "./fitter/doc/_static/fitter_256x256.png\n",
            "./fitter/doc/_static/fitter_680x680.png\n",
            "./fitter/doc/_static/fitter_64x64.png\n",
            "./fitter/doc/source/conf.py\n",
            "./fitter/doc/source/_static/copybutton.js\n",
            "./fitter/src/fitter/main.py\n",
            "./fitter/src/fitter/__init__.py\n",
            "./fitter/src/fitter/fitter.py\n",
            "./fitter/src/fitter/histfit.py\n",
            "./input_files/Calypso_101.md\n",
            "./input_files/Calypso_102.md\n",
            "./input_files/Calypso_104.md\n",
            "./input_files/Calypso_103.md\n",
            "Done file load\n",
            "Done Setup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "a3gJrp-EZqQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import google.generativeai as genai\n",
        "import json"
      ],
      "metadata": {
        "id": "wpPNq1byQPee"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = \"AIzaSyBkYYkEkbjbgNu2Z35G0RlW4RimQls4D0A\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "GDcNQLjwZteX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGHandler:\n",
        "  def __init__(self, model, db_name):\n",
        "    self.model = model\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device' : 'cpu'})\n",
        "    self.vector_db = Chroma(persist_directory=db_name, embedding_function=embedding_function)\n",
        "  def _generate_rag_prompt(self, query, context):\n",
        "    escaped = context.replace(\"'\", \"\").replace('\"', '').replace(\"\\n\", \" \")\n",
        "    open_br = \"{\"\n",
        "    close_br = \"}\"\n",
        "    prompt = (f\"\"\"\n",
        "    You are a helpful and informative bot that answers questions using text from reference context included below. \\\n",
        "    Be sure to respond in a complete sentence, being comprenhensive, including all relevant background information. \\\n",
        "    However, you are talinkg to a non-technical audience, so be sure to break down complicated concepts and \\\n",
        "    strike friendly and conversational tone. \\\n",
        "    If the context is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "    USER QUESTION: '{query}'\n",
        "    CONTEXT: '{context}'\n",
        "\n",
        "    ANSWER:\n",
        "    Each context information will have some metadata at the end of the object. \\\n",
        "    If the information from given file is relevant, use the metadata to refence \\\n",
        "    by adding the source to a list in the sources list. Use the following format, \\\n",
        "    as a json object:\n",
        "\n",
        "    {open_br}\n",
        "      answer : <your response> // string\n",
        "      sources : [\n",
        "        {open_br}\n",
        "          file_name: <Name of file>, // string\n",
        "          path: <path to source> // string\n",
        "        {close_br}\n",
        "      ] // list of path sources\n",
        "    {close_br}\n",
        "    \"\"\")\n",
        "    return prompt\n",
        "  def _get_relevant_context_from_db(self, query):\n",
        "    context = \"\"\n",
        "\n",
        "    search_results = self.vector_db.similarity_search(query, k=6)\n",
        "    for result in search_results:\n",
        "      context += result.page_content + \"\\n\"\n",
        "      context += f\"{result.metadata}\" + \"\\n\"\n",
        "    return context\n",
        "  def _generate_answer(self, prompt):\n",
        "    answer = self.model.generate_content(prompt)\n",
        "    return answer.text\n",
        "  def query(self, query):\n",
        "    context = self._get_relevant_context_from_db(query)\n",
        "    prompt  = self._generate_rag_prompt(query, context)\n",
        "    answer  = self._generate_answer(prompt)\n",
        "    answer  = json.loads(answer)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "kJv5SWlZZu1_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_handler = RAGHandler(model, \"./new_db\")"
      ],
      "metadata": {
        "id": "LrSfQJ89Zw_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c56979-c6da-4f4f-ada6-0f204484b72b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = rag_handler.query(\"According to the documentation, what's the main purpose of the standard model?\")"
      ],
      "metadata": {
        "id": "OxS4JKM0Zzcf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPe9-rsiZ0X4",
        "outputId": "34ce08a2-dbec-4ab8-c248-8f18bfe1012e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': \"The Calypso Standard Reference Model (CSRM) is a pre-configured, modular, and extensible data model used by Calypso's platform. It's designed to support trading and risk management for various financial instruments. The CSRM provides a standard way to represent financial instruments, making it easier to integrate with other systems and reducing the need for custom development. It also includes pre-built workflows, business rules, and interfaces for managing trades, valuations, and risk across different asset classes like fixed income, equities, and commodities.\", 'sources': [{'file_name': 'Calypso_104.md', 'path': './input_files/Calypso_104.md'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query[\"answer\"]"
      ],
      "metadata": {
        "id": "A_fd6l63aUw3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ce185dbe-ef74-45b7-dacf-9054be544cdc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Calypso Standard Reference Model (CSRM) is a pre-configured, modular, and extensible data model used by Calypso's platform. It's designed to support trading and risk management for various financial instruments. The CSRM provides a standard way to represent financial instruments, making it easier to integrate with other systems and reducing the need for custom development. It also includes pre-built workflows, business rules, and interfaces for managing trades, valuations, and risk across different asset classes like fixed income, equities, and commodities.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXoSBWGvRjhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}